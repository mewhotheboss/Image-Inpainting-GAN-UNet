{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "# Mount Google Drive (for saving checkpoints and outputs)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download dataset using kagglehub\n",
        "def download_dataset():\n",
        "    global path  # Make path a global variable\n",
        "    path = kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "    print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Call the function to download the dataset\n",
        "download_dataset()\n",
        "\n",
        "# Debug: List files in the dataset directory\n",
        "print(\"Files in dataset directory:\", os.listdir(path))\n",
        "\n",
        "# Check if the dataset directory exists\n",
        "if not os.path.exists(path):\n",
        "    raise FileNotFoundError(f\"Dataset directory not found at: {path}\")\n",
        "\n",
        "# Check for subdirectories (e.g., 'images', 'train', 'test')\n",
        "subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "if subdirs:\n",
        "    print(\"Subdirectories found:\", subdirs)\n",
        "    # Update the path to point to the subdirectory containing images\n",
        "    path = os.path.join(path, subdirs[0])  # Use the first subdirectory (e.g., 'images')\n",
        "    print(\"Updated dataset path:\", path)\n",
        "\n",
        "# List image files in the dataset directory\n",
        "files = [f for f in os.listdir(path) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
        "print(f\"Number of images found: {len(files)}\")  # Debug: Print the number of images\n",
        "\n",
        "# Ensure the dataset is not empty\n",
        "if len(files) == 0:\n",
        "    raise ValueError(\"No images found in the dataset directory. Please check the path.\")\n",
        "\n",
        "# Create directories for saving images and checkpoints\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/gan_unet_checkpoints\", exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HZJHDYQJW2r",
        "outputId": "3a9ebdf0-ba64-4874-ed40-1bdf8300e676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Path to dataset files: /kaggle/input/celebahq-resized-256x256\n",
            "Files in dataset directory: ['celeba_hq_256']\n",
            "Subdirectories found: ['celeba_hq_256']\n",
            "Updated dataset path: /kaggle/input/celebahq-resized-256x256/celeba_hq_256\n",
            "Number of images found: 30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net Components\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WiPMnIWOJXuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator using U-Net\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.unet = UNet(n_channels, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.unet(x)\n",
        "\n",
        "# Discriminator with Spectral Normalization\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            spectral_norm(nn.Conv2d(img_shape[0], 64, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            spectral_norm(nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)),\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, img):\n",
        "        features = self.model(img)\n",
        "        validity = self.sigmoid(torch.mean(features, dim=(2, 3)))  # Global average pooling\n",
        "        return validity"
      ],
      "metadata": {
        "id": "-e_lWt0MJZ2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Penalty\n",
        "def gradient_penalty(discriminator, real_imgs, fake_imgs):\n",
        "    alpha = torch.rand(real_imgs.size(0), 1, 1, 1).cuda()\n",
        "    interpolated = (alpha * real_imgs + (1 - alpha) * fake_imgs).requires_grad_(True)\n",
        "    validity = discriminator(interpolated)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=validity,\n",
        "        inputs=interpolated,\n",
        "        grad_outputs=torch.ones(validity.size()).cuda(),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Loss functions\n",
        "adversarial_loss = nn.BCELoss()\n",
        "pixelwise_loss = nn.L1Loss()  # L1 loss for pixel-wise reconstruction\n",
        "\n",
        "# Initialize models\n",
        "generator = UNetGenerator(3, 3)  # RGB input and output\n",
        "discriminator = Discriminator((3, 256, 256))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    pixelwise_loss.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))  # Lower LR for generator\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0004, betas=(0.5, 0.999))  # Higher LR for discriminator\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "7R9-COTJJcXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class with masking\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.files = [os.path.join(root, file) for file in os.listdir(root) if file.endswith(('png', 'jpg', 'jpeg'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.files[index]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Create a mask with a random gap\n",
        "        mask = torch.ones_like(img)\n",
        "        gap_size = 64  # Size of the gap\n",
        "        x = np.random.randint(0, 256 - gap_size)\n",
        "        y = np.random.randint(0, 256 - gap_size)\n",
        "        mask[:, x:x+gap_size, y:y+gap_size] = 0  # Create a square gap\n",
        "        masked_img = img * mask  # Apply the mask to the image\n",
        "\n",
        "        return masked_img, img  # Return masked image and original image\n",
        "\n",
        "# Create the dataset\n",
        "dataset = CustomDataset(path, transform=transform)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = 24000  # Use only 20k images for training\n",
        "test_size = len(dataset) - train_size  # Remaining images for testing\n",
        "\n",
        "# Ensure train_size is valid\n",
        "if train_size <= 0 or train_size > len(dataset):\n",
        "    raise ValueError(f\"Invalid train_size: {train_size}. Dataset has only {len(dataset)} images.\")\n",
        "\n",
        "# Perform the split\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Testing dataset size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ2yHQlSJd1F",
        "outputId": "cf529418-99ff-414b-b718-501acc169a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 24000\n",
            "Testing dataset size: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(epochs, start_epoch=0, patience=5):\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    # Debug: Print dataset size\n",
        "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Testing dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    # Debug: Print a batch from the dataloader\n",
        "    for batch in train_dataloader:\n",
        "        print(\"Batch shape:\", batch[0].shape)  # Print the shape of the masked images\n",
        "        break  # Only print the first batch\n",
        "\n",
        "    # Verify device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Move models to the correct device\n",
        "    generator.to(device)\n",
        "    discriminator.to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        epoch_g_loss = 0.0\n",
        "        epoch_d_loss = 0.0\n",
        "\n",
        "        for i, (masked_imgs, real_imgs) in enumerate(train_dataloader):\n",
        "            # Move data to the correct device\n",
        "            masked_imgs = masked_imgs.to(device)\n",
        "            real_imgs = real_imgs.to(device)\n",
        "\n",
        "            # Prepare ground truths with label smoothing\n",
        "            valid = Variable(torch.ones(real_imgs.size(0), 1).to(device) * 0.9, requires_grad=False)  # Smooth real labels\n",
        "            fake = Variable(torch.zeros(real_imgs.size(0), 1).to(device), requires_grad=False)        # Fake labels remain 0\n",
        "\n",
        "            # Train generator\n",
        "            optimizer_G.zero_grad()\n",
        "            gen_imgs = generator(masked_imgs)\n",
        "            g_adv_loss = adversarial_loss(discriminator(gen_imgs), valid)  # Adversarial loss\n",
        "            g_pixel_loss = pixelwise_loss(gen_imgs, real_imgs)  # Pixel-wise loss\n",
        "            g_loss = g_adv_loss + 10 * g_pixel_loss  # Adjusted loss weighting\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # Train discriminator\n",
        "            optimizer_D.zero_grad()\n",
        "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "            gp = gradient_penalty(discriminator, real_imgs, gen_imgs.detach())  # Gradient penalty\n",
        "            d_loss = (real_loss + fake_loss) / 2 + 1 * gp  # Reduced gradient penalty weight\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            epoch_g_loss += g_loss.item()\n",
        "            epoch_d_loss += d_loss.item()\n",
        "\n",
        "            # Print loss values for every batch\n",
        "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(train_dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}] [GP: {gp.item()}]\")\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                save_image(gen_imgs.data[:25], f\"images/{epoch}_{i}.png\", nrow=5, normalize=True)\n",
        "\n",
        "        # Calculate average losses for the epoch\n",
        "        epoch_g_loss /= len(train_dataloader)\n",
        "        epoch_d_loss /= len(train_dataloader)\n",
        "        print(f\"[Epoch {epoch}/{epochs}] [Avg D loss: {epoch_d_loss}] [Avg G loss: {epoch_g_loss}]\")\n",
        "\n",
        "        # Save checkpoints at the end of each epoch\n",
        "        save_checkpoints(epoch)\n",
        "\n",
        "        # Early stopping logic\n",
        "        if epoch_g_loss < best_loss:\n",
        "            best_loss = epoch_g_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs).\")\n",
        "            break\n",
        "\n",
        "# Save checkpoints\n",
        "def save_checkpoints(epoch):\n",
        "    torch.save(generator.state_dict(), f\"/content/drive/MyDrive/gan_unet_checkpoints/generator_epoch_{epoch}.pth\")\n",
        "    torch.save(discriminator.state_dict(), f\"/content/drive/MyDrive/gan_unet_checkpoints/discriminator_epoch_{epoch}.pth\")\n",
        "    print(f\"Checkpoints saved for epoch {epoch}.\")"
      ],
      "metadata": {
        "id": "YQsxneGSJjS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with a custom image\n",
        "def test_model_with_random_image(image_path):\n",
        "    # Verify image path\n",
        "    print(f\"Testing image path: {image_path}\")\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image not found at: {image_path}\")\n",
        "\n",
        "    # Verify checkpoint loading\n",
        "    last_saved_epoch = 49  # Update this to the last saved epoch\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/gan_unet_checkpoints/generator_epoch_{last_saved_epoch}.pth\"\n",
        "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at: {checkpoint_path}\")\n",
        "    generator.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "    # Set the generator to evaluation mode\n",
        "    generator.eval()\n",
        "\n",
        "    # Ensure consistent device usage\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    generator.to(device)\n",
        "\n",
        "    # Load the custom image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "    print(f\"Image tensor shape: {img.shape}\")  # Debug: Print image tensor shape\n",
        "\n",
        "    # Create a medium-sized mask\n",
        "    mask = torch.ones_like(img)  # Create a mask of the same shape as the image\n",
        "    gap_size = 96  # Medium-sized gap (96x96)\n",
        "    x = (img.shape[2] - gap_size) // 2  # Center the mask vertically\n",
        "    y = (img.shape[3] - gap_size) // 2  # Center the mask horizontally\n",
        "    mask[:, :, x:x+gap_size, y:y+gap_size] = 0  # Create a medium-sized square gap\n",
        "    print(f\"Mask tensor shape: {mask.shape}\")  # Debug: Print mask tensor shape\n",
        "\n",
        "    # Save the mask for visualization\n",
        "    save_image(mask, \"mask.png\", normalize=True)\n",
        "    print(\"Mask saved as 'mask.png'\")\n",
        "\n",
        "    # Apply the mask to the image\n",
        "    masked_img = img * mask  # Element-wise multiplication\n",
        "    print(f\"Masked image tensor shape: {masked_img.shape}\")  # Debug: Print masked image tensor shape\n",
        "\n",
        "    # Generate the filled image\n",
        "    with torch.no_grad():\n",
        "        filled_img = generator(masked_img)\n",
        "\n",
        "    # Save and visualize results\n",
        "    save_image(masked_img, \"masked_image.png\", normalize=True)\n",
        "    save_image(filled_img, \"filled_image.png\", normalize=True)\n",
        "    print(\"Masked and filled images saved as 'masked_image.png' and 'filled_image.png'\")\n",
        "\n",
        "    # Verify output directory\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "    if not os.path.exists(\"masked_image.png\") or not os.path.exists(\"filled_image.png\"):\n",
        "        print(\"Error: Images were not saved. Check the output directory.\")"
      ],
      "metadata": {
        "id": "7vZqIrWiJf7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Train the model for 50 epochs\n",
        "   # train(epochs=50, start_epoch=0)\n",
        "\n",
        "    # Test the model with a custom image\n",
        "    custom_image_path = \"/content/drive/MyDrive/Testing/0.jpg\"  # Update this path\n",
        "    test_model_with_random_image(custom_image_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK0ItnqDJ0yJ",
        "outputId": "dd424282-8491-49b1-c38f-dc7c7facd9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing image path: /content/drive/MyDrive/Testing/0.jpg\n",
            "Loading checkpoint from: /content/drive/MyDrive/gan_unet_checkpoints/generator_epoch_49.pth\n",
            "Image tensor shape: torch.Size([1, 3, 256, 256])\n",
            "Mask tensor shape: torch.Size([1, 3, 256, 256])\n",
            "Mask saved as 'mask.png'\n",
            "Masked image tensor shape: torch.Size([1, 3, 256, 256])\n",
            "Masked and filled images saved as 'masked_image.png' and 'filled_image.png'\n",
            "Current working directory: /content\n"
          ]
        }
      ]
    }
  ]
}